{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'formula_1', 'local']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as J\n",
    "import os\n",
    "\n",
    "def _load_nosql(database_dir):\n",
    "    dbname = os.path.basename(database_dir.rstrip(\"/\"))\n",
    "    db = client[dbname]\n",
    "    \n",
    "    for path in os.listdir(database_dir):\n",
    "        if path.endswith('.json'):\n",
    "            collection_name = path.split(\".\")[0]\n",
    "            if collection_name not in db.list_collection_names():\n",
    "                collection = db[collection_name]\n",
    "\n",
    "                collection_objs = J.load(open(os.path.join(database_dir, path)))\n",
    "                collection.insert_many(collection_objs) \n",
    "    return db\n",
    "\n",
    "\n",
    "def _documents_print(cursor):\n",
    "    cursor_list = list(cursor)\n",
    "    for d in cursor_list:\n",
    "        for k, v in d.items():\n",
    "            print(f\"{k} : {v}\", end=\" | \")\n",
    "        print(\"\\n\" + (\"*\" + \"-\"*25 + \"*\").center(125, \" \"))\n",
    "\n",
    "def _classify_integer_field(values):\n",
    "    \"\"\"\n",
    "    Classify whether an integer field is ordinal/nominal or continuous.\n",
    "    \"\"\"\n",
    "    unique_values = set(values)\n",
    "    if (len(unique_values) == len(values)) or (len(unique_values) <= 0.25*len(values)):  # Threshold can be adjusted\n",
    "        return f\"{_ordinal_nominal(values)}-cat\"\n",
    "    else:\n",
    "        return \"num\"\n",
    "\n",
    "def _uniqueness(values):\n",
    "    unique_values = set(values)\n",
    "    if len(unique_values) == len(values):\n",
    "        return \"unique\"\n",
    "    else:\n",
    "        return \"non-unique\"\n",
    "\n",
    "def _ordinal_nominal(values):\n",
    "    if len(values) == 1:\n",
    "        return 'ordinal'\n",
    "    \n",
    "    vsorted =  sorted(values)   \n",
    "    vmin, vmax, d, n = vsorted[0], vsorted[-1], vsorted[1]-vsorted[0], len(values)-1\n",
    "\n",
    "    return ('ordinal' if (n*d == vmax-vmin) else 'nominal')\n",
    "\n",
    "\n",
    "from collections import defaultdict \n",
    "def _get_schema(db, collection_name):\n",
    "\n",
    "    if os.path.exists(f'./NoSQL/{db.name}/schemas/{collection_name}.json'):\n",
    "        return dict(J.load(open(f'./NoSQL/{db.name}/schemas/{collection_name}.json')))\n",
    "    else:\n",
    "        os.makedirs(f'./NoSQL/{db.name}/schemas', exist_ok=True)\n",
    "    \n",
    "    schema = defaultdict(lambda: [0, '', '', 'unique', 'non-null', -1, -1])  #count data-type, conpt-type, uniqueness, nullity, minvalue, maxvalue\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    int_values = defaultdict(list)\n",
    "    key_values = defaultdict(list)\n",
    "   \n",
    "    # Sample documents to infer schema\n",
    "    sample_docs = collection.find()\n",
    "    \n",
    "    for doc in sample_docs:\n",
    "        for key, value in doc.items():\n",
    "            field_type = type(value).__name__\n",
    "            schema[key][0] += 1\n",
    "            if schema[key][1] in ['', 'NoneType']:\n",
    "                schema[key][1] = field_type\n",
    "            \n",
    "            if field_type == 'NoneType':\n",
    "                schema[key][4] = 'null'\n",
    "            \n",
    "            if field_type == 'int':\n",
    "                int_values[key].append(value)\n",
    "            \n",
    "            key_values[key].append(value)\n",
    "    \n",
    "    for key, values in int_values.items():\n",
    "        classification = _classify_integer_field(values)\n",
    "        schema[key][2] = classification # = schema[key].pop('int')\n",
    "        \n",
    "    \n",
    "    for key, values in key_values.items():\n",
    "        uniqueness_value = _uniqueness(values)\n",
    "        \n",
    "        schema[key][3] = uniqueness_value  # = schema[key].pop('int')\n",
    "\n",
    "        if schema[key][1] in ['str', 'dict', 'ObjectId']:\n",
    "                schema[key][2] = 'cat'    \n",
    "        elif schema[key][1] in ['datetime', 'float']:\n",
    "                schema[key][2] = 'num'\n",
    "                schema[key][5], schema[key][6] = min(values), max(values)\n",
    "        elif schema[key][1] in ['NoneType']:\n",
    "                schema[key][2] = 'null'\n",
    "        else:\n",
    "            schema[key][5], schema[key][6] = min(values), max(values)\n",
    "            pass \n",
    "\n",
    "    \n",
    "    J.dump(schema, open(f'./NoSQL/{db.name}/schemas/{collection_name}.json', 'w'))\n",
    "    return schema\n",
    "\n",
    "def _describe_database_schema(db):\n",
    "    database_schema = {}\n",
    "    print(f\"Database : {db.name}\")\n",
    "    for collection_name in db.list_collection_names():\n",
    "        #print(f\"Describing schema for collection: {collection_name}\")\n",
    "        schema = _get_schema(db, collection_name)\n",
    "        database_schema[collection_name] = schema\n",
    "    \n",
    "    return database_schema\n",
    "\n",
    "def _schema_print(schema_description):\n",
    "    # Print the schema\n",
    "    for collection, schema in schema_description.items():\n",
    "        print(f\"Collection: {collection}\")\n",
    "        for field, data_list in schema.items():\n",
    "            print(f\"  Field: {field}\")\n",
    "            count, typee, zone, uniqueness, nullity, minval, maxval = data_list\n",
    "            print(f\"    Data-Type: {typee}, Count: {count}, Concpt-Type: {zone}, Unique: {True if uniqueness == 'unique' else False}, Null: {True if nullity == 'null' else False} \" + (f\"Min: {minval}, Max: {maxval}\" if zone=='num' else \"\"))\n",
    "\n",
    "def data_ingest(file_path='NoSQL/formula_1'):\n",
    "    _load_nosql(file_path)\n",
    "    return \n",
    "\n",
    "def data_explore(file_path='NoSQL/formula_1'):\n",
    "    db = _load_nosql(file_path)\n",
    "    schema_description = _describe_database_schema(db)\n",
    "    _schema_print(schema_description)\n",
    "    return schema_description\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_sql(schema_description):\n",
    "    order_queries = defaultdict(lambda : defaultdict(set))\n",
    "    for collection in schema_description:\n",
    "        query_pattern = {'cmd' : ['SELECT'],\n",
    "                        'cat' : [\"\"], 'num':[\"\"], 'nom-cat':[\"\"], 'ord-cat':[\"\"], \n",
    "                        'agg': ['SUM', 'COUNT', 'AVG', 'MIN', 'MAX', 'STDDEV', 'VARIANCE', 'MEDIAN'], \n",
    "                        'clause':['WHERE', 'GROUP BY', 'ORDER BY'],\n",
    "                        'numeric_op' : ['>', '<', '>=', '<=', '='] }\n",
    "        \n",
    "        for col in schema_description[collection]:\n",
    "            _type = schema_description[collection][col][2]\n",
    "            \n",
    "            if _type  == 'cat':\n",
    "                query_pattern['cat'] += [col]\n",
    "            elif _type  == 'nominal-cat':\n",
    "                query_pattern['nom-cat'] += [col]\n",
    "            elif _type  == 'ordinal-cat':\n",
    "                query_pattern['ord-cat'] += [col]\n",
    "            else:\n",
    "                query_pattern['num'] += [col]\n",
    "        \n",
    "       \n",
    "        n = random.randint(10, 15)\n",
    "        _ = 0\n",
    "\n",
    "        \n",
    "\n",
    "        while _ < n:\n",
    "            query = \"\"\n",
    "            cmd = random.choice(query_pattern['cmd'])\n",
    "            query += cmd + \" \"\n",
    "\n",
    "            agg = random.choice(query_pattern['agg'] + [\"\"])\n",
    "            \n",
    "            if agg == \"\":\n",
    "                _A_ = random.choice(query_pattern['cat'] + query_pattern['num']+ query_pattern['nom-cat'] + query_pattern['ord-cat'])\n",
    "                if _A_ == \"\":\n",
    "                    continue\n",
    "                query += _A_ + f\" FROM {collection} \"\n",
    "            elif agg == \"COUNT\":\n",
    "                _A_ = random.choice(query_pattern['cat'] + query_pattern['num']+ query_pattern['nom-cat'] + query_pattern['ord-cat'] + [\"*\"])\n",
    "                if _A_ == \"\":\n",
    "                    continue\n",
    "                query += f\"{agg}({_A_}) FROM {collection} \"\n",
    "            elif agg == \"MIN\":\n",
    "                _A_ = random.choice(query_pattern['num']+ query_pattern['ord-cat'])\n",
    "                if _A_ == \"\":\n",
    "                    continue\n",
    "                query += f\"{agg}({_A_}) FROM {collection} \"\n",
    "            elif agg == \"MAX\":\n",
    "                _A_ = random.choice(query_pattern['num']+ query_pattern['ord-cat'])\n",
    "                if _A_ == \"\":\n",
    "                    continue\n",
    "                query += f\"{agg}({_A_}) FROM {collection} \"\n",
    "            elif agg == \"MEDIAN\":\n",
    "                _A_ = random.choice(query_pattern['num']+ query_pattern['ord-cat'])\n",
    "                if _A_ == \"\":\n",
    "                    continue\n",
    "                query += f\"{agg}({_A_}) FROM {collection} \"\n",
    "            else:\n",
    "                num = random.choice(query_pattern['num'])\n",
    "                query += f\"{agg}({num}) FROM {collection} \"\n",
    "                if num == \"\":\n",
    "                    continue\n",
    "            \n",
    "            clause = random.choice(query_pattern['clause']+[\"\"])\n",
    "\n",
    "            if clause == \"\":\n",
    "                pass\n",
    "            elif clause == \"WHERE\":\n",
    "                num = random.choice(query_pattern['num'])\n",
    "                if num == \"\":\n",
    "                    continue\n",
    "                nop = random.choice(query_pattern['numeric_op'])\n",
    "                numv = random.randint(schema_description[collection][col][5], schema_description[collection][col][6]+1)\n",
    "                query += f\"WHERE {num} {nop} {numv} \"\n",
    "            elif clause == \"ORDER BY\":\n",
    "                num = random.choice(query_pattern['num'] + query_pattern['ord-cat'])\n",
    "                if num == \"\":\n",
    "                    continue\n",
    "                query += f\"ORDER BY {num}\"\n",
    "            elif clause == \"GROUP BY\":\n",
    "                cat = random.choice(query_pattern['cat'] + query_pattern['nom-cat'])\n",
    "                if cat == \"\":\n",
    "                    continue\n",
    "                query += f\"GROUP BY {cat}\"\n",
    "\n",
    "            \n",
    "            order_queries[agg][clause].add(query)\n",
    "               \n",
    "        \n",
    "            _ += 1\n",
    "    \n",
    "    order_queries = dict(order_queries)\n",
    "    \n",
    "    \n",
    "    for agg, clauses in order_queries.items():\n",
    "        clauses = dict(clauses)\n",
    "        for clause, queries in clauses.items():\n",
    "            print(f\"{agg} | {clause}\", end=\"\\n\\t\")\n",
    "            for query in queries:\n",
    "                print(query, end=\"\\n\\t\")\n",
    "            print(\"\")\n",
    "            print(\"-\"*100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database : formula_1\n",
      "Collection: constructors\n",
      "  Field: _id\n",
      "    Data-Type: ObjectId, Count: 208, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: constructorId\n",
      "    Data-Type: int, Count: 208, Concpt-Type: nominal-cat, Unique: True, Null: False \n",
      "  Field: constructorRef\n",
      "    Data-Type: str, Count: 208, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: name\n",
      "    Data-Type: str, Count: 208, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: nationality\n",
      "    Data-Type: str, Count: 208, Concpt-Type: cat, Unique: False, Null: False \n",
      "  Field: url\n",
      "    Data-Type: str, Count: 208, Concpt-Type: cat, Unique: False, Null: False \n",
      "Collection: circuits\n",
      "  Field: _id\n",
      "    Data-Type: ObjectId, Count: 72, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: circuitId\n",
      "    Data-Type: int, Count: 72, Concpt-Type: ordinal-cat, Unique: True, Null: False \n",
      "  Field: circuitRef\n",
      "    Data-Type: str, Count: 72, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: name\n",
      "    Data-Type: str, Count: 72, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: location\n",
      "    Data-Type: str, Count: 72, Concpt-Type: cat, Unique: False, Null: False \n",
      "  Field: country\n",
      "    Data-Type: str, Count: 72, Concpt-Type: cat, Unique: False, Null: False \n",
      "  Field: lat\n",
      "    Data-Type: float, Count: 72, Concpt-Type: num, Unique: False, Null: False Min: -34.9272, Max: 57.2653\n",
      "  Field: lng\n",
      "    Data-Type: float, Count: 72, Concpt-Type: num, Unique: False, Null: False Min: -118.189, Max: 138.927\n",
      "  Field: alt\n",
      "    Data-Type: NoneType, Count: 72, Concpt-Type: null, Unique: False, Null: True \n",
      "  Field: url\n",
      "    Data-Type: str, Count: 72, Concpt-Type: cat, Unique: True, Null: False \n",
      "Collection: constructorStandings\n",
      "  Field: _id\n",
      "    Data-Type: ObjectId, Count: 11836, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: constructorStandingsId\n",
      "    Data-Type: int, Count: 11836, Concpt-Type: nominal-cat, Unique: True, Null: False \n",
      "  Field: raceId\n",
      "    Data-Type: int, Count: 11836, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "  Field: constructorId\n",
      "    Data-Type: int, Count: 11836, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "  Field: points\n",
      "    Data-Type: float, Count: 11836, Concpt-Type: num, Unique: False, Null: False Min: 0.0, Max: 765.0\n",
      "  Field: position\n",
      "    Data-Type: int, Count: 11836, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "  Field: positionText\n",
      "    Data-Type: str, Count: 11836, Concpt-Type: cat, Unique: False, Null: False \n",
      "  Field: wins\n",
      "    Data-Type: int, Count: 11836, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "Collection: races\n",
      "  Field: _id\n",
      "    Data-Type: ObjectId, Count: 976, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: raceId\n",
      "    Data-Type: int, Count: 976, Concpt-Type: nominal-cat, Unique: True, Null: False \n",
      "  Field: year\n",
      "    Data-Type: int, Count: 976, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "  Field: round\n",
      "    Data-Type: int, Count: 976, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "  Field: circuitId\n",
      "    Data-Type: int, Count: 976, Concpt-Type: nominal-cat, Unique: False, Null: False \n",
      "  Field: name\n",
      "    Data-Type: str, Count: 976, Concpt-Type: cat, Unique: False, Null: False \n",
      "  Field: date\n",
      "    Data-Type: str, Count: 976, Concpt-Type: cat, Unique: True, Null: False \n",
      "  Field: time\n",
      "    Data-Type: str, Count: 976, Concpt-Type: cat, Unique: False, Null: True \n",
      "  Field: url\n",
      "    Data-Type: str, Count: 976, Concpt-Type: cat, Unique: True, Null: False \n"
     ]
    }
   ],
   "source": [
    "schema_description=data_explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT | GROUP BY\n",
      "\tSELECT COUNT(*) FROM races GROUP BY url\n",
      "\tSELECT COUNT(date) FROM races GROUP BY circuitId\n",
      "\tSELECT COUNT(constructorId) FROM constructorStandings GROUP BY wins\n",
      "\tSELECT COUNT(*) FROM constructors GROUP BY constructorRef\n",
      "\tSELECT COUNT(_id) FROM constructors GROUP BY name\n",
      "\tSELECT COUNT(name) FROM races GROUP BY date\n",
      "\tSELECT COUNT(name) FROM constructors GROUP BY constructorRef\n",
      "\tSELECT COUNT(name) FROM constructors GROUP BY url\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COUNT | \n",
      "\tSELECT COUNT(url) FROM constructors \n",
      "\tSELECT COUNT(nationality) FROM constructors \n",
      "\tSELECT COUNT(date) FROM races \n",
      "\tSELECT COUNT(constructorRef) FROM constructors \n",
      "\tSELECT COUNT(url) FROM races \n",
      "\tSELECT COUNT(*) FROM races \n",
      "\tSELECT COUNT(constructorId) FROM constructors \n",
      "\tSELECT COUNT(raceId) FROM races \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COUNT | WHERE\n",
      "\tSELECT COUNT(raceId) FROM constructorStandings WHERE points >= 15 \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "COUNT | ORDER BY\n",
      "\tSELECT COUNT(constructorId) FROM constructorStandings ORDER BY points\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      " | \n",
      "\tSELECT constructorRef FROM constructors \n",
      "\tSELECT position FROM constructorStandings \n",
      "\tSELECT year FROM races \n",
      "\tSELECT constructorId FROM constructorStandings \n",
      "\tSELECT lat FROM circuits \n",
      "\tSELECT url FROM races \n",
      "\tSELECT constructorId FROM constructors \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      " | GROUP BY\n",
      "\tSELECT url FROM races GROUP BY circuitId\n",
      "\tSELECT url FROM constructors GROUP BY url\n",
      "\tSELECT round FROM races GROUP BY raceId\n",
      "\tSELECT year FROM races GROUP BY year\n",
      "\tSELECT raceId FROM races GROUP BY time\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      " | ORDER BY\n",
      "\tSELECT circuitRef FROM circuits ORDER BY circuitId\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VARIANCE | GROUP BY\n",
      "\tSELECT VARIANCE(lat) FROM circuits GROUP BY location\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VARIANCE | WHERE\n",
      "\tSELECT VARIANCE(alt) FROM circuits WHERE lat > -1 \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VARIANCE | \n",
      "\tSELECT VARIANCE(alt) FROM circuits \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VARIANCE | ORDER BY\n",
      "\tSELECT VARIANCE(points) FROM constructorStandings ORDER BY points\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "STDDEV | ORDER BY\n",
      "\tSELECT STDDEV(alt) FROM circuits ORDER BY alt\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "STDDEV | \n",
      "\tSELECT STDDEV(points) FROM constructorStandings \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "AVG | GROUP BY\n",
      "\tSELECT AVG(alt) FROM circuits GROUP BY location\n",
      "\tSELECT AVG(lat) FROM circuits GROUP BY _id\n",
      "\tSELECT AVG(points) FROM constructorStandings GROUP BY positionText\n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "AVG | \n",
      "\tSELECT AVG(points) FROM constructorStandings \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MAX | \n",
      "\tSELECT MAX(circuitId) FROM circuits \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MIN | \n",
      "\tSELECT MIN(points) FROM constructorStandings \n",
      "\t\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generate_sql(schema_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "inp = \"find all names of races happened in year 2009\"\n",
    "\n",
    "lemmatized_inp = \" \".join([lemmatizer.lemmatize(word) for word in inp.split()])\n",
    "\n",
    "import re \n",
    "\n",
    "actions = get_datamuse_synonyms('select')\n",
    "aggs = []\n",
    "for word in ['summation', 'minimum', 'maximum', 'average', 'median', 'count', 'variance', 'standard deviation']:\n",
    "    aggs += [lemmatizer.lemmatize(synword) for synword in get_datamuse_synonyms(word)]\n",
    "\n",
    "\n",
    "cols = ['names', 'date', 'time', 'round']\n",
    "cols = [lemmatizer.lemmatize(word) for word in cols] #\n",
    "\n",
    "lemmatized_cols = \" \".join([lemmatizer.lemmatize(word) for word in cols])\n",
    "\n",
    "pattern = f'(?P<action>{\"|\".join(actions)}).*(?P<agg>{\"|\".join(aggs)})?.*(?P<col>{\"|\".join(cols)}).*(?P<table>races).*((?P<clause>\\w+)(?P<cond>\\w+))?'\n",
    "\n",
    "matches = re.search(f'{pattern}', inp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "Expected Keyword 'select', found 'find'  (at char 0), (line:1, col:1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Example parsing\u001b[39;00m\n\u001b[1;32m     29\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind all names of races happened in year 2009\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 30\u001b[0m parsed_result \u001b[38;5;241m=\u001b[39m \u001b[43mselect_stmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(parsed_result\u001b[38;5;241m.\u001b[39mdump())\n",
      "File \u001b[0;32m~/Documents/Grad-Docs/Third Sem/FDM/ChatDB/.venv/lib/python3.11/site-packages/pyparsing/util.py:376\u001b[0m, in \u001b[0;36mreplaced_by_pep8.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m#     f\"Deprecated - use {fn.__name__}\", DeprecationWarning, stacklevel=2\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Grad-Docs/Third Sem/FDM/ChatDB/.venv/lib/python3.11/site-packages/pyparsing/core.py:1216\u001b[0m, in \u001b[0;36mParserElement.parse_string\u001b[0;34m(self, instring, parse_all, parseAll)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;66;03m# catch and re-raise exception from here, clearing out pyparsing internal stack trace\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mParseException\u001b[0m: Expected Keyword 'select', found 'find'  (at char 0), (line:1, col:1)"
     ]
    }
   ],
   "source": [
    "from pyparsing import Word, alphas, nums, oneOf, Combine, Keyword, Group, Optional\n",
    "\n",
    "# Define basic SQL keywords\n",
    "select_keyword = Keyword(\"select\", caseless=True)\n",
    "from_keyword = Keyword(\"from\", caseless=True)\n",
    "where_keyword = Keyword(\"where\", caseless=True)\n",
    "and_keyword = Keyword(\"and\", caseless=True)\n",
    "or_keyword = Keyword(\"or\", caseless=True)\n",
    "\n",
    "# Define basic components\n",
    "column_name = Word(alphas + \"_\")\n",
    "table_name = Word(alphas + \"_\")\n",
    "integer = Word(nums)\n",
    "comparison_operator = oneOf(\"= != < > <= >=\")\n",
    "\n",
    "# Grammar for SELECT statement\n",
    "select_stmt = (\n",
    "    select_keyword \n",
    "    + column_name.setResultsName(\"column\") \n",
    "    + from_keyword \n",
    "    + table_name.setResultsName(\"table\")\n",
    "    + Optional(where_keyword \n",
    "               + column_name.setResultsName(\"where_column\") \n",
    "               + comparison_operator.setResultsName(\"operator\") \n",
    "               + integer.setResultsName(\"value\"))\n",
    ")\n",
    "\n",
    "# Example parsing\n",
    "query = \"find all names of races happened in year 2009\"\n",
    "parsed_result = select_stmt.parseString(query)\n",
    "\n",
    "print(parsed_result.dump())\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "\n",
    "# Define a simple grammar\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> SELECT CLAUSE FROM TABLE WHERE_COND\n",
    "    SELECT -> \"select\"\n",
    "    CLAUSE -> COLUMN\n",
    "    COLUMN -> \"age\" | \"name\" | \"salary\" \n",
    "    FROM -> \"from\"\n",
    "    TABLE -> \"users\" | \"employees\"\n",
    "    WHERE_COND -> \"where\" CONDITION\n",
    "    CONDITION -> COLUMN OPERATOR VALUE\n",
    "    OPERATOR -> \"=\" | \">\" | \"<\"\n",
    "    VALUE -> \"21\" | \"1000\"\n",
    "\"\"\")\n",
    "\n",
    "# Define a parser\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# Parse the input\n",
    "query = \"select age from users where age > 21\"\n",
    "tokens = query.split()\n",
    "\n",
    "# Parse and print results\n",
    "for tree in parser.parse(tokens):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "inp = \"find all names of races happened in year 2009\"\n",
    "\n",
    "lemmatized_inp = \" \".join([lemmatizer.lemmatize(word) for word in inp.split()])\n",
    "\n",
    "import re \n",
    "\n",
    "actions = get_datamuse_synonyms('select')\n",
    "aggs = []\n",
    "for word in ['summation', 'minimum', 'maximum', 'average', 'median', 'count', 'variance', 'standard deviation']:\n",
    "    aggs += [lemmatizer.lemmatize(synword) for synword in get_datamuse_synonyms(word)]\n",
    "\n",
    "\n",
    "cols = ['names', 'date', 'time', 'round']\n",
    "cols = [lemmatizer.lemmatize(word) for word in cols] #\n",
    "\n",
    "lemmatized_cols = \" \".join([lemmatizer.lemmatize(word) for word in cols])\n",
    "\n",
    "pattern = f'(?P<action>{\"|\".join(actions)}).*(?P<agg>{\"|\".join(aggs)})?.*(?P<col>{\"|\".join(cols)}).*(?P<table>races).*((?P<clause>\\w+)(?P<cond>\\w+))?'\n",
    "\n",
    "matches = re.search(f'{pattern}', inp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResults(['h'], {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_or_hi = W\n",
    "hello_or_hi.parse_string(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyparsing\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing\n",
      "Successfully installed pyparsing-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyparsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02040816326530612\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_datamuse_synonyms(word):\n",
    "    url = f\"https://api.datamuse.com/words?ml={word}&v=enwiki\"\n",
    "    response = requests.get(url).json()\n",
    "    synonyms = [item['word'] for item in response]\n",
    "    return synonyms\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "def find_common_related_words(word1, word2):\n",
    "    # Get related words for each input word\n",
    "    url1 = f\"https://api.datamuse.com/words?ml={word1}\"\n",
    "    url2 = f\"https://api.datamuse.com/words?ml={word2}\"\n",
    "    \n",
    "    response1 = requests.get(url1).json()\n",
    "    response2 = requests.get(url2).json()\n",
    "    \n",
    "    # Extract related words\n",
    "    related1 = {item['word'] for item in response1}\n",
    "    related2 = {item['word'] for item in response2}\n",
    "    \n",
    "    # Find common related words\n",
    "    common_related = related1.intersection(related2)\n",
    "    total = related1.union(related2)\n",
    "    \n",
    "    return len(common_related)/len(total)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranu/Documents/Grad-Docs/Third Sem/FDM/ChatDB/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/samruddhim/imdb-movies-analysis?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.2M/15.2M [00:00<00:00, 16.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/pranu/.cache/kagglehub/datasets/samruddhim/imdb-movies-analysis/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samruddhim/imdb-movies-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /Users/pranu/.cache/kagglehub/datasets/samruddhim/imdb-movies-analysis/versions/1 NoSQL/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to NoSQL/movie_info/cleaned_comments.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def clean_mongo_json(data):\n",
    "    \"\"\"Recursively clean MongoDB JSON by removing '$'-prefixed keys.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key.startswith('$') or key.startswith(\"_id\"):\n",
    "                del data[key]\n",
    "                continue\n",
    "            else:\n",
    "                while isinstance(value, dict):\n",
    "                    value = list(value.values())[0]\n",
    "                    pass\n",
    "            \n",
    "                data[key] = value\n",
    "    \n",
    "\n",
    "    return data\n",
    "    \n",
    "\n",
    "# Load your MongoDB sample data\n",
    "input_file = 'NoSQL/movie_info/comments.json'\n",
    "output_file = 'NoSQL/movie_info/cleaned_comments.json'\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "    # Load JSON data from the file\n",
    "    mongo_data = file.readlines()\n",
    "    for i in range(len(mongo_data)):\n",
    "        mongo_data[i] = json.loads(mongo_data[i])\n",
    "\n",
    "# Clean the MongoDB JSON\n",
    "cleaned_data = clean_mongo_json(mongo_data)\n",
    "\n",
    "# Save the cleaned data to a new file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(cleaned_data, file, indent=4)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-15T20:02:04.941-0700\tconnected to: mongodb://localhost/\n",
      "2024-10-15T20:02:05.810-0700\t50304 document(s) imported successfully. 0 document(s) failed to import.\n"
     ]
    }
   ],
   "source": [
    "!mongoimport --db movies_info --collection comments --file NoSQL/movie_info/comments.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
